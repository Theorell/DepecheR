#' Performing optimization and penalized K-means clustering
#' 
#'
#' This function is a user-friendly wrapper integrating the dOpt and dClust functions. It only requires a dataset and an id vector. It starts by doing all necessary optimizations, both on the smallest sample size that is needed to perform the most stable clustering, and to identify the optimal penalty. It then performs clustering based on the values identified in the optimization step. 
#' @importFrom moments kurtosis
#' @param inDataFrame A dataframe or matrix with the data that will be used to create the clustering. Cytometry data should be transformed using biexponential, arcsinh transformation or similar, and day-to-day normalizations need to be performed for all data. Scaling, etc, is on the other hand performed within the function. 
#' @param penalties A single value, a vector of values, or possibly a list of two vectors, if dual clustering is performed. This argument decides whether a single penalty will be used for clustering, or if multiple penalties will be evaluated to identify the optimal one. The suggested default values are empirically defined and might not be optimal for a specific dataset, but the algorithm will warn if the most optimal values are on the borders of the range. Note that when this offset is 0, there is no penalization, which means that the algorithm runs standard K-means clustering.
#' @param sampleSizes This controls what fraction of the dataset that will be used to run the penalty optimization. A single value or a vector of values are accepted. "default" results in the full file in files up to 10000 events. From larger files, a sample of 10000 random rows is used. A user specified number is accepted.
#' @param selectionSampleSize The size of the dataset used to find the optimal solution out of the many generated by the penalty optimization at each sample size. "default" results in the full file in files up to 10000 events. In cases where the sampleSizes argument is larger than 10000, default leads to the generation of a random subset to the same size also for the selectionSampleSize. A user specified number is also accepted.
#' @param dualClustSetup A dataframe with two columns: the first specifying which step (1 or 2) the variable should be included in, the second specifying the column name for the variable in question. Iit is used if a two-step clustering should be performed, eg in the case where phenotypic clustering should be performed, followed by clustering on functional variables.
#' @param k Number of initial cluster centers. The higher the number, the greater the precision of the clustering, but the computing time is also increased with the number of starting points. Default is 20. If penalties=0, k-means clustering with k clusters will be performed.
#' @param minARIImprovement This is the stop criterion for the penalty optimization algorithm: the more iterations that are run, the smaller will the improvement of the corrected Rand index be, and this sets the threshold when the inner iterations stop. Defaults to 0.01, or 1 percent. 
#' @param maxIter The maximal number of iterations that are performed in the penalty optimization.
#' @param minARI This is the stop criterion for the iterative optimization of the sample size: the maximum corrected Rand index that is acceptable. Defaults to 0.05, or 5 percent difference between sets of two full dataset allocations based on clusterings of a certain sample size.
#' @param ids Optionally, a vector of the same length as rows in the inDataFrameScaled can be included. If so, it is used to generate a final analysis, where a table of the fraction of observations for each individual and each cluster is created.
#' @param returnProcessedInData If the scaled and centered data should be returned. Defaults to TRUE.
#' @param log2Off In cases with extreme tails, the clustering algorithm log2-transforms the data by default. This can be turned off using this command.  
#' @param multiCoreScaling For internal dScale. If the algorithm should be performed on multiple cores. This increases speed in situations when very large datasets (eg >1 000 000 rows) are scaled. With smaller datasets, it works, but is slow. Defaults to FALSE.
#' @seealso \code{\link{dAllocate}}, \code{\link{dScale}}
#' @return A nested list with varying components depending on the setup above:
#' \describe{
#'    \item{clusterVector}{A vector with the same length as number of rows in the inDataFrameUsed, where the cluster identity of each observation is noted.}
#'    \item{clusterCenters}{A matrix containing information about where the centers are in all the variables that contributed to creating the cluster with the given penalty term. Is used by dAllocate.}
#'    \item{penaltyOptList}{A list of two dataframes:
#'    \describe{
#'              \item{penaltyOpt.df}{A one row dataframe with the settings for the optimal penalty.}
#'              \item{meanOptimDf}{A dataframe with the information about the results with all tested penalty values.}
#'            }
#'     }
#'     \item{sampleSizeOptList}{This is only included if multiple sample sizes are run. It is a dataframe, in which each row represents one sample size, and in which the last row is thus the chosen, optimal sample size. It has the following columns:
#'     \describe{
#'               \item{SampleSize}{This column shows the sample size of each boot strap subsampling in the optimization procedure.}
#'               \item{Lowest distance}{This vector shows the optimal stability, expressed as the lowest distance between the bootstrap subsampling runs at each of the boot strap subsamling sizes.}
#'               \item{Improvement}{Here, the improvement, expressed as a fraction between 0 and 1 is shown. When the improvement is less than minARIImprovement, the algorithm automatically stops.}
#'              } 
#'     }
#'     \item{idClusterFractions}{If a valid ids vector is included, this dataframe is returned that contains the what fraction of each id that is present in each cluster. Calculated on a per id basis.}
#'     \item{scaledInData}{If returnProcessedInData is TRUE, this slot will contain the scaled and centered indata.}

#' } If a dual setup is used, the result will be a nested list, where the first sublist with the information above of the result of the primary clustering and the following list components are the result of all the secondary clusterings combined.
#' @examples
#' #Generate a default size dataframe with bimodally distributed data
#' x <- generateBimodalData(samplings=2, dataCols=12)
#'
#' #Set a reasonable working directory, e.g.
#' setwd("~/Desktop")
#'
#' #First, just run with the standard settings
#' xDepecheObject <- depeche(x[,2:ncol(x)], sampleSizes=500, maxIter=20)
#'
#' #Look at the result
#' str(xDepecheObject)
#' 
#' #Now, a dual cluster setup is attempted
#' xDepecheObject <- depeche(x[,2:ncol(x)], dualClustSetup=data.frame(rep(1:2, each=6), colnames(x_scaled)), penalties=c(64, 128), sampleSizes=500, selectionSampleSize=500, maxIter=20, ids=x[,1])
#'
#' #Look at the result
#' str(xDepecheObject)
#' 
#' @export depeche
depeche <- function(inDataFrame, dualClustSetup, penalties=c(2^0, 2^0.5, 2^1, 2^1.5, 2^2, 2^2.5, 2^3, 2^3.5, 2^4, 2^4.5, 2^5), sampleSizes="default", selectionSampleSize="default", k=20, minARIImprovement=0.01, minARI=0.99, maxIter=200, ids, returnProcessedInData=FALSE, log2Off=FALSE, multiCoreScaling=FALSE){

  if(class(inDataFrame)=="matrix"){
    inDataFrame <- as.data.frame.matrix(inDataFrame)
  }

  #Here it is checked if the data has very extreme tails, and if so, the data is log2 transformed
  if(log2Off==FALSE && kurtosis(as.vector(as.matrix(inDataFrame)))>100){
    kurtosisValue1 <- kurtosis(as.vector(as.matrix(inDataFrame)))
    inDataFrame <- log2(inDataFrame+1)
    kurtosisValue2 <- kurtosis(as.vector(as.matrix(inDataFrame)))
    print(paste("The data was found to be very strongly skewed (kurtosis", kurtosisValue1, "), so it was log2-transformed before clustering, leading to a new kurtosis value of", kurtosisValue2, ". Turn this off using the log2Off parameter if you dislike it."))
  }
  

  #Scaling is performed
  if(ncol(inDataFrame)<100){
    print("As the dataset has less than 100 columns, it is assumed that it contains protein information. Peak centering is therefore used.")

    inDataFramePreScaled <- dScale(inDataFrame, scale=FALSE, center="peak", multiCore=multiCoreScaling)
    #Here, all the data is divided by the standard deviation of the full dataset
    sdInDataFramePreScaled <- sd(as.matrix(inDataFramePreScaled))
    inDataFrameScaled <- inDataFramePreScaled/sdInDataFramePreScaled
  } else {
    print("As the dataset has more than 100 columns, it is assumed that it contains single cell RNA sequencing information, and mean centering is applied.")
      inDataFramePreScaled <- scale(inDataFrame, scale=FALSE)

    #Here, all the data is divided by the standard deviation of the full dataset
    sdInDataFramePreScaled <- sd(as.matrix(inDataFramePreScaled))
    inDataFrameScaled <- as.data.frame(inDataFramePreScaled/sdInDataFramePreScaled)
  }
  
  if(missing(dualClustSetup)==TRUE){
    if(missing(ids)==TRUE){
      dClustResult <- dClustCoFunction(inDataFrameScaled, firstClusterNumber=1, penalties=penalties, sampleSizes=sampleSizes, selectionSampleSize=selectionSampleSize, k=k, minARIImprovement=minARIImprovement, minARI=minARI, maxIter=maxIter)
    } else {
      dClustResult <- dClustCoFunction(inDataFrameScaled, firstClusterNumber=1, penalties=penalties, sampleSizes=sampleSizes, selectionSampleSize=selectionSampleSize, k=k, minARIImprovement=minARIImprovement, minARI=minARI, maxIter=maxIter, ids=ids)
    }
    if(returnProcessedInData==TRUE){
      resultLengt <- length(dClustResult)+1
      dClustResult[[resultLengt]] <- inDataFrameScaled
    }
    return(dClustResult)
  }
  
  #Here, the dual cluster setup is created
  if(missing(dualClustSetup)==FALSE){
    inDataColumns <- dualClustSetup[,2]
    inDataFrameFirst <- inDataFrameScaled[as.character(inDataColumns[which(dualClustSetup[,1]==1)])]
    if(is.list(penalties)==FALSE){
      penaltyList <- list(penalties, penalties)
    }
    if(missing(ids)==TRUE){
    dClustResultFirst <- dClustCoFunction(inDataFrameFirst, penalties=penaltyList[[1]], sampleSizes=sampleSizes, selectionSampleSize=selectionSampleSize, k=k, minARIImprovement=minARIImprovement, minARI=minARI, maxIter=maxIter)
    } else {
      dClustResultFirst <- dClustCoFunction(inDataFrameFirst, penalties=penalties, sampleSizes=sampleSizes, selectionSampleSize=selectionSampleSize, k=k, minARIImprovement=minARIImprovement, minARI=minARI, maxIter=maxIter, ids=ids)
    }
    
    print(paste("Done with level one clustering where ", length(unique(dClustResultFirst$clusterVector)), " clusters were created. Now initiating level two.", sep=""))
    
    #After this first step, clustering is performed within each of the clusters produced by the dClustResultFirst
    inDataFrameSecond <- inDataFrameScaled[as.character(inDataColumns[which(dualClustSetup[,1]==2)])]
    inDataFrameSecondList <- list()
    for(i in 1:length(unique(dClustResultFirst$clusterVector))){
      inDataFrameSecondList[[i]] <- inDataFrameSecond[which(dClustResultFirst$clusterVector==i),]
    }
    
    #Now create the list of cluster numbers
    firstClusterNumberList <- list()
    for(i in 1:length(unique(dClustResultFirst$clusterVector))){
      rightNumberSize <- 100*i
      firstClusterNumberList[[i]] <- rightNumberSize+1
    }
    #Here, the secondary clusters are generated for each subframe created by the primary clusters
    dClustResultSecondList <- mapply(dClustCoFunction, inDataFrameSecondList, firstClusterNumberList, MoreArgs=list(penalties=penaltyList[[2]], sampleSizes=sampleSizes, selectionSampleSize=selectionSampleSize, k=k, minARIImprovement=minARIImprovement, minARI=minARI, maxIter=maxIter), SIMPLIFY=FALSE)

    #Now, all the clustering data is recompiled to one long cluster vector
    complexClusterVector <- inDataFrameScaled[,1]
    for(i in 1:length(dClustResultSecondList)){
      complexClusterVector[which(dClustResultFirst$clusterVector==i)] <- dClustResultSecondList[[i]][[1]]
    }
    
    #And the cluster centers are also compiled
    clusterCentersList <- list()
    for(i in 1:length(dClustResultSecondList)){
      clusterCentersList[[i]] <- dClustResultSecondList[[i]][[2]]
    }
    
    #Create a list of all unique colnames
    colnamesList <- list()
    for(i in 1:length(dClustResultSecondList)){
      colnamesList[[i]] <- colnames(clusterCentersList[[i]])
    }
    uniqueColnamesVector <- sort(unique(unlist(colnamesList)))
    
    #Now, if a variable is missing in a certain cluster center matrix, it is added with zeros. ALso the variables are sorted. 
    for(i in 1:length(clusterCentersList)){
      if(length(clusterCentersList[[i]])!=length(uniqueColnamesVector)){
        missingColnames <- uniqueColnamesVector[!uniqueColnamesVector %in% colnames(clusterCentersList[[i]])]
        zeroDataFrame <-  as.data.frame(matrix(0, nrow=nrow(clusterCentersList[[i]]), ncol=length(missingColnames)))
        colnames(zeroDataFrame) <- missingColnames
        clusterCentersList[[i]] <- cbind(clusterCentersList[[i]], zeroDataFrame)
      }
    
      clusterCentersList[[i]] <- clusterCentersList[[i]][ , order(colnames(clusterCentersList[[i]]))]
      
    }
    
    secondLevelClusterCenters <- do.call("rbind", clusterCentersList)
    
    #And after all these centers have been compiled, the fist set of clusster centers are also included
    firstClusterCenters <- dClustResultFirst$clusterCenters
    firstOnSecondClusterCentersList <- list()
    clusterClusters <- substr(as.character(row.names(secondLevelClusterCenters)), 1, 1)
    for(i in 1:length(clusterClusters)){
      firstOnSecondClusterCentersList[[i]] <- firstClusterCenters[which(row.names(firstClusterCenters)==clusterClusters[i]),]
    }
    
    firstOnSecondClusterCenters <- do.call("rbind", firstOnSecondClusterCentersList)
    colnames(firstOnSecondClusterCenters) <- colnames(firstClusterCenters)
    row.names(firstOnSecondClusterCenters) <- row.names(secondLevelClusterCenters)
    
    #And finally, these new columns are added to the complexClusterCenters
    complexClusterCenters <- data.frame(firstOnSecondClusterCenters, secondLevelClusterCenters)
      
    #And now, all the penalty optimization and possible sample size optimizations are saved
    penaltyOptListList <- do.call("list", sapply(dClustResultSecondList, "[[", 3))

    dClustResult <- list("levelOneCLusterResult"=dClustResultFirst, "levelTwoClusterVector"=complexClusterVector, "levelTwoClusterCenters"=complexClusterCenters, "levelTwoPenaltyOptList"=penaltyOptListList)
    
    if(length(sampleSizes)>1){
      sampleSizeOptList <- do.call("list", sapply(dClustResultSecondList, "[[", 4))
      nextClustResultPosition <- length(dClustResult)+1
      dClustResult[[nextClustResultPosition]] <- as.data.frame.matrix(sampleSizeOptList)
      names(dClustResult)[[length(dClustResult)]] <- "levelTwoSampleSizeOptList"
      }
    
    #And finally, if a valid ids vector is included, the percentages are calculated here
    if(missing(ids)==FALSE && length(ids)==nrow(inDataFrameScaled)){
      
      clusterTable <- table(complexClusterVector, ids)
      
      countTable <- table(ids)
      
      clusterFractionsForAllIds <- clusterTable
      
      for(i in 1:length(countTable)){
        x <- clusterTable[,i]/countTable[i]
        clusterFractionsForAllIds[,i] <- x
      }
      
      nextClustResultPosition <- length(dClustResult)+1
      dClustResult[[nextClustResultPosition]] <- as.data.frame.matrix(clusterFractionsForAllIds)
      names(dClustResult)[[length(dClustResult)]] <- "levelTwoIdClusterFractions"
      
    }
    
    if(returnProcessedInData==TRUE){
      resultLength <- length(dClustResult)+1
      dClustResult[[resultLength]] <- inDataFrameScaled
    }
    
    return(dClustResult)
  }
  

}
  